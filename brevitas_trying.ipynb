{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import IntBias\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# import brevitas\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFoldDataset(Dataset):\n",
    "    def __init__(self, data_files, target_files):\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for data_file, target_file in zip(data_files, target_files):\n",
    "            data = np.load(data_file)\n",
    "            targets = np.load(target_file)\n",
    "            \n",
    "            self.data.append(data)\n",
    "            self.targets.append(targets)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fold_idx = index // len(self.data[0])\n",
    "        item_idx = index % len(self.data[0])\n",
    "        \n",
    "        data = self.data[fold_idx][item_idx]\n",
    "        target = self.targets[fold_idx][item_idx]\n",
    "        \n",
    "        return torch.from_numpy(data), torch.from_numpy(target)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) * len(self.data[0])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the base file name and number of folds\n",
    "# base_file_name = 'jet_images_c8_minpt2_allfeats_robust_fast'\n",
    "base_file_name = 'jet_images_c8_minpt2_ptetaphi_robust_fast'\n",
    "num_folds = 5\n",
    "\n",
    "def datamaker(base_file_name, num_folds, val_fold, batch_size=32):\n",
    "    # Generate the file paths for your k-fold data and targets\n",
    "    train_folds = [i for i in range(num_folds) if i != val_fold]\n",
    "    val_folds = [val_fold]\n",
    "    train_data_files = [f'./normalized_data2/x_{base_file_name}_{i}.npy' for i in train_folds]\n",
    "    train_target_files = [f'./normalized_data2/y_{base_file_name}_{i}.npy' for i in train_folds]\n",
    "    val_data_files = [f'./normalized_data2/x_{base_file_name}_{i}.npy' for i in val_folds]\n",
    "    val_target_files = [f'./normalized_data2/y_{base_file_name}_{i}.npy' for i in val_folds]\n",
    "    \n",
    "    train_dataset = KFoldDataset(train_data_files, train_target_files)\n",
    "    val_dataset = KFoldDataset(val_data_files, val_target_files)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_loader, val_loader = datamaker(base_file_name, num_folds, 3)\n",
    "\n",
    "def test_loader_maker(base_file_name, batch_size=32):\n",
    "    # /Users/dimademler/Documents/GitHub/Morph-DeepSets/normalized_data/y_jet_images_c8_minpt2_allfeats_robust_fast_test.npy\n",
    "    test_data_file = f'./normalized_data2/x_{base_file_name}_test.npy'\n",
    "    test_target_file = f'./normalized_data2/y_{base_file_name}_test.npy'\n",
    "\n",
    "    test_dataset = KFoldDataset([test_data_file], [test_target_file])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        for test_data, test_targets in test_loader:\n",
    "            test_data = test_data.to(device).float()\n",
    "            test_targets = test_targets.to(device).float()\n",
    "            \n",
    "            outputs = model(test_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true_labels = torch.argmax(test_targets, 1)  # Get the true class labels\n",
    "            \n",
    "            test_total += true_labels.size(0)\n",
    "            test_correct += (predicted == true_labels).sum().item()\n",
    "        \n",
    "        test_accuracy = test_correct / test_total\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSetsInv(nn.Module):\n",
    "    def __init__(self, input_size, nnodes_phi: int = 32, nnodes_rho: int = 16, activ: str = \"relu\"):\n",
    "        super(DeepSetsInv, self).__init__()\n",
    "        self.nclasses = 5\n",
    "        self.phi = nn.Sequential(\n",
    "            qnn.QuantLinear(input_size, nnodes_phi, bias=True, weight_bit_width=8),\n",
    "            self.get_activation(activ),\n",
    "            qnn.QuantLinear(nnodes_phi, nnodes_phi, bias=True, weight_bit_width=8),\n",
    "            self.get_activation(activ),\n",
    "            qnn.QuantLinear(nnodes_phi, nnodes_phi, bias=True, weight_bit_width=8),\n",
    "            self.get_activation(activ),\n",
    "        )\n",
    "        self.rho = nn.Sequential(\n",
    "            qnn.QuantLinear(nnodes_phi, nnodes_rho, bias=True, weight_bit_width=8),\n",
    "            self.get_activation(activ),\n",
    "            qnn.QuantLinear(nnodes_rho, self.nclasses, bias=True, weight_bit_width=8),\n",
    "        )\n",
    "\n",
    "    def get_activation(self, activ):\n",
    "        if activ == \"relu\":\n",
    "            return qnn.QuantReLU(bit_width=8)\n",
    "        elif activ == \"sigmoid\":\n",
    "            return qnn.QuantSigmoid(bit_width=8)\n",
    "        elif activ == \"tanh\":\n",
    "            return qnn.QuantTanh(bit_width=8)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activ}\")\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        phi_output = self.phi(inputs)\n",
    "        # print(\"phi_output dtype:\", phi_output.dtype)\n",
    "        sum_output = torch.mean(phi_output, dim=1)\n",
    "        # print( \"sum_output dtype:\", sum_output.dtype)\n",
    "        rho_output = self.rho(sum_output)\n",
    "        # print( \"rho_output dtype:\", rho_output.dtype)\n",
    "        return rho_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/morph_deepsets/lib/python3.10/site-packages/torch/_tensor.py:1394: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403226260/work/c10/core/TensorImpl.h:1911.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 1.0948, Val Loss: 1.0455, Val Accuracy: 0.5979\n",
      "Epoch 2/100, Train Loss: 1.0492, Val Loss: 1.0396, Val Accuracy: 0.6009\n",
      "Epoch 3/100, Train Loss: 1.0388, Val Loss: 1.0503, Val Accuracy: 0.5883\n",
      "Epoch 4/100, Train Loss: 1.0336, Val Loss: 1.0319, Val Accuracy: 0.6034\n",
      "Epoch 5/100, Train Loss: 1.0322, Val Loss: 1.0241, Val Accuracy: 0.6094\n",
      "Epoch 6/100, Train Loss: 1.0304, Val Loss: 1.0281, Val Accuracy: 0.6078\n",
      "Epoch 7/100, Train Loss: 1.0299, Val Loss: 1.0284, Val Accuracy: 0.6048\n",
      "Epoch 8/100, Train Loss: 1.0289, Val Loss: 1.0228, Val Accuracy: 0.6091\n",
      "Epoch 9/100, Train Loss: 1.0281, Val Loss: 1.0261, Val Accuracy: 0.6025\n",
      "Epoch 10/100, Train Loss: 1.0115, Val Loss: 1.0124, Val Accuracy: 0.6144\n",
      "Epoch 11/100, Train Loss: 1.0105, Val Loss: 1.0130, Val Accuracy: 0.6133\n",
      "Epoch 12/100, Train Loss: 1.0107, Val Loss: 1.0142, Val Accuracy: 0.6127\n",
      "Epoch 13/100, Train Loss: 1.0101, Val Loss: 1.0122, Val Accuracy: 0.6160\n",
      "Epoch 14/100, Train Loss: 1.0100, Val Loss: 1.0127, Val Accuracy: 0.6136\n",
      "Epoch 15/100, Train Loss: 1.0097, Val Loss: 1.0149, Val Accuracy: 0.6148\n",
      "Epoch 16/100, Train Loss: 1.0096, Val Loss: 1.0121, Val Accuracy: 0.6143\n",
      "Epoch 17/100, Train Loss: 1.0094, Val Loss: 1.0136, Val Accuracy: 0.6144\n",
      "Epoch 18/100, Train Loss: 1.0072, Val Loss: 1.0113, Val Accuracy: 0.6159\n",
      "Epoch 19/100, Train Loss: 1.0072, Val Loss: 1.0116, Val Accuracy: 0.6151\n",
      "Epoch 20/100, Train Loss: 1.0072, Val Loss: 1.0110, Val Accuracy: 0.6164\n",
      "Epoch 21/100, Train Loss: 1.0071, Val Loss: 1.0109, Val Accuracy: 0.6154\n",
      "Epoch 22/100, Train Loss: 1.0071, Val Loss: 1.0110, Val Accuracy: 0.6156\n",
      "Epoch 23/100, Train Loss: 1.0071, Val Loss: 1.0108, Val Accuracy: 0.6159\n",
      "Epoch 24/100, Train Loss: 1.0073, Val Loss: 1.0111, Val Accuracy: 0.6155\n",
      "Epoch 25/100, Train Loss: 1.0067, Val Loss: 1.0107, Val Accuracy: 0.6162\n",
      "Epoch 26/100, Train Loss: 1.0068, Val Loss: 1.0105, Val Accuracy: 0.6164\n",
      "Epoch 27/100, Train Loss: 1.0067, Val Loss: 1.0107, Val Accuracy: 0.6166\n",
      "Epoch 28/100, Train Loss: 1.0068, Val Loss: 1.0111, Val Accuracy: 0.6162\n",
      "Epoch 29/100, Train Loss: 1.0069, Val Loss: 1.0118, Val Accuracy: 0.6155\n",
      "Epoch 30/100, Train Loss: 1.0069, Val Loss: 1.0111, Val Accuracy: 0.6167\n",
      "Epoch 31/100, Train Loss: 1.0069, Val Loss: 1.0106, Val Accuracy: 0.6163\n",
      "Epoch 32/100, Train Loss: 1.0068, Val Loss: 1.0110, Val Accuracy: 0.6161\n",
      "Epoch 33/100, Train Loss: 1.0071, Val Loss: 1.0104, Val Accuracy: 0.6163\n",
      "Epoch 34/100, Train Loss: 1.0070, Val Loss: 1.0108, Val Accuracy: 0.6167\n",
      "Epoch 35/100, Train Loss: 1.0066, Val Loss: 1.0105, Val Accuracy: 0.6165\n",
      "Epoch 36/100, Train Loss: 1.0065, Val Loss: 1.0109, Val Accuracy: 0.6158\n",
      "Epoch 37/100, Train Loss: 1.0066, Val Loss: 1.0115, Val Accuracy: 0.6158\n",
      "Epoch 38/100, Train Loss: 1.0067, Val Loss: 1.0105, Val Accuracy: 0.6165\n",
      "Epoch 39/100, Train Loss: 1.0062, Val Loss: 1.0103, Val Accuracy: 0.6168\n",
      "Epoch 40/100, Train Loss: 1.0064, Val Loss: 1.0105, Val Accuracy: 0.6161\n",
      "Epoch 41/100, Train Loss: 1.0064, Val Loss: 1.0119, Val Accuracy: 0.6157\n",
      "Epoch 42/100, Train Loss: 1.0064, Val Loss: 1.0103, Val Accuracy: 0.6168\n",
      "Epoch 43/100, Train Loss: 1.0063, Val Loss: 1.0103, Val Accuracy: 0.6168\n",
      "Epoch 44/100, Train Loss: 1.0062, Val Loss: 1.0103, Val Accuracy: 0.6168\n",
      "Epoch 45/100, Train Loss: 1.0062, Val Loss: 1.0103, Val Accuracy: 0.6167\n",
      "Epoch 46/100, Train Loss: 1.0062, Val Loss: 1.0103, Val Accuracy: 0.6168\n",
      "Early stopping: Validation accuracy has not increased in 7 epochs\n",
      "Test Accuracy: 0.6142\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "input_size = 3  # Assuming each input feature vector has a size of 16\n",
    "model = DeepSetsInv(input_size=input_size, nnodes_phi=32, nnodes_rho=16, activ=\"relu\")\n",
    "# model.qconfig = IntBias.IntQuant.UINT8\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0032)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 100\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "best_val_accuracy = 0.0\n",
    "patience= 7\n",
    "for epoch in range(num_epochs):\n",
    "    lossval = 0.0\n",
    "    model.train() \n",
    "    for batch_data, batch_targets in train_loader:\n",
    "        batch_data = batch_data.to(device).float()\n",
    "        batch_targets = batch_targets.to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_data)\n",
    "        \n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        lossval += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        for val_data, val_targets in val_loader:\n",
    "            val_data = val_data.to(device).float()\n",
    "            val_targets = val_targets.to(device).float()\n",
    "            \n",
    "            outputs = model(val_data)\n",
    "            loss = criterion(outputs, val_targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true_labels = torch.argmax(val_targets, 1)  # Get the true class labels\n",
    "            \n",
    "            val_total += true_labels.size(0)\n",
    "            val_correct += (predicted == true_labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter > patience:\n",
    "                print(f\"Early stopping: Validation accuracy has not increased in {patience} epochs\")\n",
    "                break\n",
    "\n",
    "        # Update the learning rate based on validation accuracy\n",
    "        lr_scheduler.step(val_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {lossval/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "test_loader = test_loader_maker(base_file_name)\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DeepSetsInv(input_size=input_size, nnodes_phi=32, nnodes_rho=16, activ=\"relu\")\n",
    "# model.qconfig = brevitas.quant.IntQuant.UINT8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "morph_deepsets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
