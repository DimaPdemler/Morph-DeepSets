{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import IntBias\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# import brevitas\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DeepSetDataset(Dataset):\n",
    "    def __init__(self, data_files, target_files):\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        for data_file, target_file in zip(data_files, target_files):\n",
    "            data = np.load(data_file)\n",
    "            targets = np.load(target_file)\n",
    "            self.data.append(data)\n",
    "            self.targets.append(targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fold_idx = index // len(self.data[0])\n",
    "        item_idx = index % len(self.data[0])\n",
    "        data = self.data[fold_idx][item_idx]\n",
    "        target = self.targets[fold_idx][item_idx]\n",
    "        return torch.from_numpy(data), torch.from_numpy(target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) * len(self.data[0])\n",
    "\n",
    "base_file_name = 'jet_images_c8_minpt2_ptetaphi_robust_fast'\n",
    "\n",
    "def datamaker(base_file_name, batch_size=32, val_split=0.2):\n",
    "    train_data_files = [f'./normalized_data3/x_train_{base_file_name}.npy']\n",
    "    train_target_files = [f'./normalized_data3/y_train_{base_file_name}.npy']\n",
    "    test_data_files = [f'./normalized_data3/x_test_{base_file_name}.npy']\n",
    "    test_target_files = [f'./normalized_data3/y_test_{base_file_name}.npy']\n",
    "\n",
    "    train_tempdataset = DeepSetDataset(train_data_files, train_target_files)\n",
    "\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(train_tempdataset)),\n",
    "        test_size=val_split,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(train_tempdataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(train_tempdataset, val_indices)\n",
    "    test_dataset = DeepSetDataset(test_data_files, test_target_files)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# class DeepSetDataset(Dataset):\n",
    "#     def __init__(self, data_files, target_files):\n",
    "#         self.data = []\n",
    "#         self.targets = []\n",
    "        \n",
    "#         for data_file, target_file in zip(data_files, target_files):\n",
    "#             data = np.load(data_file)\n",
    "#             targets = np.load(target_file)\n",
    "            \n",
    "#             self.data.append(data)\n",
    "#             self.targets.append(targets)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         fold_idx = index // len(self.data[0])\n",
    "#         item_idx = index % len(self.data[0])\n",
    "        \n",
    "#         data = self.data[fold_idx][item_idx]\n",
    "#         target = self.targets[fold_idx][item_idx]\n",
    "        \n",
    "#         return torch.from_numpy(data), torch.from_numpy(target)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data) * len(self.data[0])\n",
    "\n",
    "\n",
    "\n",
    "# # Define the base file name and number of folds\n",
    "# # base_file_name = 'jet_images_c8_minpt2_allfeats_robust_fast'\n",
    "# base_file_name = 'jet_images_c8_minpt2_ptetaphi_robust_fast'\n",
    "\n",
    "# def datamaker(base_file_name, batch_size=32):\n",
    "#     train_data_files = [f'./normalized_data3/x_train_{base_file_name}.npy']\n",
    "#     train_target_files = [f'./normalized_data3/y_train_{base_file_name}.npy']\n",
    "#     test_data_files = [f'./normalized_data3/x_test_{base_file_name}.npy']\n",
    "#     test_target_files = [f'./normalized_data3/y_test_{base_file_name}.npy']\n",
    "\n",
    "#     train_tempdataset = DeepSetDataset(train_data_files, train_target_files)\n",
    "#     test_dataset = DeepSetDataset(test_data_files, test_target_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Define the base file name and number of folds\n",
    "# # base_file_name = 'jet_images_c8_minpt2_allfeats_robust_fast'\n",
    "# base_file_name = 'jet_images_c8_minpt2_ptetaphi_robust_fast'\n",
    "\n",
    "# # Split the training data into training and validation sets\n",
    "# train_data, val_data, train_targets, val_targets = train_test_split(train_dataset.data, train_dataset.targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create the training and validation datasets\n",
    "# train_dataset = DeepSetDataset(train_data, train_targets)\n",
    "# val_dataset = DeepSetDataset(val_data, val_targets)\n",
    "\n",
    "# # Create the training and validation data loaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "train_loader, val_loader, test_loader = datamaker(base_file_name, batch_size=32)\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        for test_data, test_targets in test_loader:\n",
    "            test_data = test_data.to(device).float()\n",
    "            test_targets = test_targets.to(device).float()\n",
    "            \n",
    "            outputs = model(test_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true_labels = torch.argmax(test_targets, 1)  # Get the true class labels\n",
    "            \n",
    "            test_total += true_labels.size(0)\n",
    "            test_correct += (predicted == true_labels).sum().item()\n",
    "        \n",
    "        test_accuracy = test_correct / test_total\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSetsInv(nn.Module):\n",
    "    def __init__(self, input_size, nnodes_phi: int = 32, nnodes_rho: int = 16, activ: str = \"relu\"):\n",
    "        super(DeepSetsInv, self).__init__()\n",
    "        self.nclasses = 5\n",
    "        self.phi = nn.Sequential(\n",
    "            qnn.QuantLinear(input_size, nnodes_phi, bias=True, weight_bit_width=8),\n",
    "            self.get_activation(activ),\n",
    "            qnn.QuantLinear(nnodes_phi, nnodes_phi, bias=True, weight_bit_width=8),\n",
    "            self.get_activation(activ),\n",
    "            qnn.QuantLinear(nnodes_phi, nnodes_phi, bias=True, weight_bit_width=8),\n",
    "            self.get_activation(activ),\n",
    "        )\n",
    "        self.rho = nn.Sequential(\n",
    "            qnn.QuantLinear(nnodes_phi, nnodes_rho, bias=True, weight_bit_width=8),\n",
    "            self.get_activation(activ),\n",
    "            qnn.QuantLinear(nnodes_rho, self.nclasses, bias=True, weight_bit_width=8),\n",
    "        )\n",
    "\n",
    "    def get_activation(self, activ):\n",
    "        if activ == \"relu\":\n",
    "            return qnn.QuantReLU(bit_width=8)\n",
    "        elif activ == \"sigmoid\":\n",
    "            return qnn.QuantSigmoid(bit_width=8)\n",
    "        elif activ == \"tanh\":\n",
    "            return qnn.QuantTanh(bit_width=8)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activ}\")\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        phi_output = self.phi(inputs)\n",
    "        # print(\"phi_output dtype:\", phi_output.dtype)\n",
    "        sum_output = torch.mean(phi_output, dim=1)\n",
    "        # print( \"sum_output dtype:\", sum_output.dtype)\n",
    "        rho_output = self.rho(sum_output)\n",
    "        # print( \"rho_output dtype:\", rho_output.dtype)\n",
    "        return rho_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/morph_deepsets/lib/python3.10/site-packages/torch/_tensor.py:1394: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403226260/work/c10/core/TensorImpl.h:1911.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Train Loss: 1.0810, Val Loss: 1.0299, Val Accuracy: 0.5991\n",
      "Epoch 2/2, Train Loss: 1.0275, Val Loss: 1.0194, Val Accuracy: 0.6073\n",
      "Test Accuracy: 0.6074\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "input_size = 3  # Assuming each input feature vector has a size of 16\n",
    "model = DeepSetsInv(input_size=input_size, nnodes_phi=32, nnodes_rho=16, activ=\"relu\")\n",
    "# model.qconfig = IntBias.IntQuant.UINT8\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0032)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 2\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "best_val_accuracy = 0.0\n",
    "patience= 7\n",
    "for epoch in range(num_epochs):\n",
    "    lossval = 0.0\n",
    "    model.train() \n",
    "    for batch_data, batch_targets in train_loader:\n",
    "        batch_data = batch_data.to(device).float()\n",
    "        batch_targets = batch_targets.to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_data)\n",
    "        \n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        lossval += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        for val_data, val_targets in val_loader:\n",
    "            val_data = val_data.to(device).float()\n",
    "            val_targets = val_targets.to(device).float()\n",
    "            \n",
    "            outputs = model(val_data)\n",
    "            loss = criterion(outputs, val_targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true_labels = torch.argmax(val_targets, 1)  # Get the true class labels\n",
    "            \n",
    "            val_total += true_labels.size(0)\n",
    "            val_correct += (predicted == true_labels).sum().item()\n",
    "\n",
    "        val_accuracy = val_correct / val_total\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter > patience:\n",
    "                print(f\"Early stopping: Validation accuracy has not increased in {patience} epochs\")\n",
    "                break\n",
    "\n",
    "        # Update the learning rate based on validation accuracy\n",
    "        lr_scheduler.step(val_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {lossval/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DeepSetsInv(input_size=input_size, nnodes_phi=32, nnodes_rho=16, activ=\"relu\")\n",
    "# model.qconfig = brevitas.quant.IntQuant.UINT8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "morph_deepsets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
